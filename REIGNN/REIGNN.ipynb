{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec30eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y torch\n",
    "!pip install torch==1.8.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install --no-index --no-cache-dir torch-scatter  torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.1+cu101.html\n",
    "!pip install --no-cache-dir torch-cluster -f https://pytorch-geometric.com/whl/torch-1.8.1+cu101.html\n",
    "!pip install --no-cache-dir torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.8.1+cu101.html\n",
    "!pip install --no-cache-dir torch-geometric\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71084aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import ModuleList, Embedding\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv,GATv2Conv\n",
    "from torch_geometric.nn import PNAConv, CGConv, BatchNorm, global_add_pool\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b07358",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7fbdc0",
   "metadata": {},
   "source": [
    "General data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_2_root = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2021dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_edges_papers_general = pd.read_csv(path_2_root + \"processed_data/SSORC_CS_2010_2021_authors_edges_papers_indices.csv\", index_col = 0, \\\n",
    "                                   converters={\"papers_indices\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "authors_edges_general = pd.read_csv(path_2_root + \"processed_data/SSORC_CS_2010_2021_authors_edge_list.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c6ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_features_general = pd.read_csv(path_2_root + \"processed_data/SSORC_CS_2010_2021_papers_features_vectorized_compressed_32.csv\", index_col = 0)\n",
    "authors_features_general = pd.read_csv(path_2_root + \"processed_data/SSORC_CS_2010_2021_authors_features.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf1ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aev = authors_edges_general.values\n",
    "edge_to_index = {(aev[i][0], aev[i][1]):i for i in tqdm(range(len(aev)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293e8ce0",
   "metadata": {},
   "source": [
    "Local data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5279a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'SSORC_CS_10_21_1437_3164_unfiltered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62326e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_edges_papers = pd.read_csv(path_2_root + \"datasets/\" + dataset_name + \"/\" + dataset_name + \"_\" + \"authors_edges_papers_indices.csv\", index_col = 0,\\\n",
    "                                   converters={\"papers_indices\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b1b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_graph = nx.read_edgelist(path_2_root + \"datasets/\" + dataset_name + \"/\" + dataset_name + \"_\" + \"authors.edgelist\", create_using = nx.DiGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7799d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_graph = nx.read_edgelist(path_2_root + \"datasets/\" + dataset_name + \"/\" + dataset_name + \"_\" + \"papers.edgelist\", create_using = nx.DiGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26620c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_targets = pd.read_csv(path_2_root + \"datasets/\" + dataset_name + \"/\" + dataset_name + \"_papers_targets.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62226204",
   "metadata": {},
   "source": [
    "### MANDATORY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efee2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sAe = list(authors_graph.edges)\n",
    "sAe = [(int(sAe[i][0]), int(sAe[i][1])) for i in range(len(sAe))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab38e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_edges_papers_sub_2 = [authors_edges_papers[\"papers_indices\"][edge_to_index[sAe[i]]] for i in tqdm(range(len(sAe)))]\n",
    "authors_edges_papers_sub_flat_2 = [int(item) for subarray in authors_edges_papers_sub_2 for item in subarray]\n",
    "unique_papers_2 = list(set(authors_edges_papers_sub_flat_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgn = list(citation_graph.nodes())\n",
    "cgn = [int(cgn[i]) for i in range(len(cgn))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a5ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(unique_papers_2).intersection(set(cgn))), len(unique_papers_2), len(cgn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa35f9",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c342cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = \"10\"\n",
    "path = path_2_root + \"datasets/\" + dataset_name + \"/split_\" + str(splits) + \"/\"  \n",
    "\n",
    "split = 0\n",
    "train_data_a = torch.load(path + dataset_name + '_train_sample_' + str(split) + '.data')\n",
    "val_data_a = torch.load(path + dataset_name + '_val_sample_' + str(split) + '.data')\n",
    "test_data_a = torch.load(path + dataset_name + '_test_sample_' + str(split) + '.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c8b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_nodes = list(citation_graph.nodes)\n",
    "papers_nodes = [int(papers_nodes[i]) for i in range(len(papers_nodes))]\n",
    "papers_node_features = papers_features_general.iloc[papers_nodes, :]\n",
    "for node in tqdm(citation_graph.nodes):\n",
    "    citation_graph.nodes[node]['x'] = list(papers_node_features.loc[[int(node)]].values[0])\n",
    "authors_nodes = list(authors_graph.nodes)\n",
    "authors_nodes = [int(authors_nodes[i]) for i in range(len(authors_nodes))]\n",
    "authors_node_features = authors_features_general.loc[authors_nodes]\n",
    "for node in tqdm(authors_graph.nodes):\n",
    "    authors_graph.nodes[node]['x'] = list(authors_node_features.loc[[int(node)]].values[0])\n",
    "data_author = from_networkx(authors_graph)\n",
    "data_citation = from_networkx(citation_graph)\n",
    "\n",
    "deg = torch.zeros(2, dtype=torch.long)\n",
    "d = degree(train_data_a.edge_index[1], num_nodes=train_data_a.num_nodes, dtype=torch.long)\n",
    "deg = torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "train_data_a.x, val_data_a.x, test_data_a.x = data_author.x.float(), data_author.x.float(), data_author.x.float()\n",
    "data_citation.x = data_citation.x.float()\n",
    "\n",
    "original_a_nodes = list(authors_graph.nodes)\n",
    "pyg_id_2_original_id = {i:int(original_a_nodes[i]) for i in range(len(original_a_nodes))}\n",
    "\n",
    "\n",
    "original_a_nodes = list(authors_graph.nodes)\n",
    "pyg_id_2_original_id = {i:int(original_a_nodes[i]) for i in range(len(original_a_nodes))}\n",
    "\n",
    "sAe_t = train_data_a.edge_index.cpu().numpy().T\n",
    "sAe_t = [(pyg_id_2_original_id[int(sAe_t[i][0])], pyg_id_2_original_id[int(sAe_t[i][1])]) for i in range(len(sAe_t))]\n",
    "\n",
    "authors_edges_papers_sub_2t = [authors_edges_papers[\"papers_indices\"][edge_to_index[sAe_t[i]]] for i in tqdm(range(len(sAe_t)))]\n",
    "authors_edges_papers_sub_flat_2t = [str(item) for subarray in authors_edges_papers_sub_2t for item in subarray]\n",
    "unique_papers_2t = list(set(authors_edges_papers_sub_flat_2t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75117aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_graph_sub = citation_graph.subgraph(unique_papers_2t)\n",
    "citation_graph_sub_nodes = list(citation_graph_sub.nodes())\n",
    "global_to_local_id_citation = {citation_graph_sub_nodes[i]:i for i in range(len(citation_graph_sub_nodes))}\n",
    "authors_graph_sub_nodes = list(authors_graph.nodes())\n",
    "global_to_local_id_authors = {authors_graph_sub_nodes[i]:i for i in range(len(authors_graph_sub_nodes))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_to_papers = dict()\n",
    "for i in tqdm(range(len(sAe_t))):\n",
    "    papers = authors_edges_papers_sub_2t[i]\n",
    "    author_1, author_2 = sAe_t[i]\n",
    "    for author in sAe_t[i]:\n",
    "        if author in authors_to_papers:\n",
    "            for paper in papers:\n",
    "                authors_to_papers[global_to_local_id_authors[str(author)]].add(global_to_local_id_citation[paper])\n",
    "        else:\n",
    "            authors_to_papers[global_to_local_id_authors[str(author)]] = set()\n",
    "            for paper in papers:\n",
    "                authors_to_papers[global_to_local_id_authors[str(author)]].add(global_to_local_id_citation[paper])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915051f3",
   "metadata": {},
   "source": [
    "### Final preparations of graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9452582",
   "metadata": {},
   "source": [
    "Adding feature description of citation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a1f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in tqdm(citation_graph_sub.nodes):\n",
    "    citation_graph_sub.nodes[node]['x'] = list(papers_features_general.loc[[int(node)]].values[0])\n",
    "data_citation = from_networkx(citation_graph_sub)\n",
    "data_citation.x = data_citation.x.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db694e1",
   "metadata": {},
   "source": [
    "Adding feature description of co-authorship graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b2812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_nodes = list(authors_graph.nodes)\n",
    "authors_nodes = [int(authors_nodes[i]) for i in range(len(authors_nodes))]\n",
    "authors_node_features = authors_features_general.loc[authors_nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b98bab6",
   "metadata": {},
   "source": [
    "### Auxiliary targets computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eeb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_author = data_author \n",
    "edges_ordered = [(int(data_author.edge_index.T[i][0]), int(data_author.edge_index.T[i][1])) for i in range(len(data_author.edge_index.T))]\n",
    "index_to_edge = {i:edges_ordered[i] for i in range(len(edges_ordered))}\n",
    "authors_edges_papers_sample = authors_edges_papers_sub_2\n",
    "citation_nodes = list(citation_graph.nodes)\n",
    "ownership_dict = {}\n",
    "inds_dict = {}\n",
    "for i in tqdm(range(len(authors_edges_papers_sample))):\n",
    "    arr = authors_edges_papers_sample[i]\n",
    "    collab_embeddings = []\n",
    "    for j in range(len(arr)):\n",
    "        ind = citation_nodes.index(arr[j]) # index_outer_2_index_inner[int(arr[j])]\n",
    "        collab_embeddings.append(ind)\n",
    "    ownership_dict[i] = i\n",
    "    inds_dict[i] = collab_embeddings\n",
    "    \n",
    "embs_dict = inds_dict\n",
    "lens = set([len(embs_dict[i]) for i in range(len(embs_dict))])\n",
    "batch_dict_x = {}\n",
    "batch_dict_owner = {}\n",
    "batch_dict_ind = {}\n",
    "for i in tqdm(range(len(embs_dict))):\n",
    "    if (len(embs_dict[i])) in batch_dict_x:\n",
    "        batch_dict_x[len(embs_dict[i])].append(embs_dict[i])\n",
    "        batch_dict_owner[len(embs_dict[i])].append(ownership_dict[i])\n",
    "        batch_dict_ind[len(embs_dict[i])].append(i)\n",
    "    else:\n",
    "        batch_dict_x[len(embs_dict[i])], batch_dict_owner[len(embs_dict[i])], batch_dict_ind[len(embs_dict[i])] = [], [], []\n",
    "        batch_dict_x[len(embs_dict[i])].append(embs_dict[i])\n",
    "        batch_dict_owner[len(embs_dict[i])].append(ownership_dict[i])\n",
    "        batch_dict_ind[len(embs_dict[i])].append(i)\n",
    "\n",
    "for length in batch_dict_owner:\n",
    "    batch_dict_owner[length] = [index_to_edge[batch_dict_owner[length][i]] for i in range(len(batch_dict_owner[length]))]\n",
    "    \n",
    "batch_list_x = list(batch_dict_x.values())\n",
    "batch_list_owner = list(batch_dict_owner.values())\n",
    "batch_list_ind = list(batch_dict_ind.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_targets = papers_targets.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_targets = []\n",
    "for i in tqdm(range(len(batch_list_x))):\n",
    "    batch = batch_list_x[i]\n",
    "    values = []\n",
    "    for j in range(len(batch)):\n",
    "        values = [papers_targets[batch[j][k]] for k in range(len(batch[j]))]\n",
    "        values = np.array(values).T\n",
    "        targets = [max(values[0]), sum(values[1])/len(values[1]), \n",
    "                   sum(values[2])/len(values[2]), sum(values[3])/len(values[3]),\n",
    "                   len(values[0])]\n",
    "        aux_targets.append(targets)\n",
    "        \n",
    "batch_list_owner_flat = [edge for batch in batch_list_owner for edge in batch]\n",
    "aux_target_dict = {batch_list_owner_flat[i]:aux_targets[i] for i in tqdm(range(len(aux_targets)))}\n",
    "\n",
    "train_edges_aux_t, val_edges_aux_t, test_edges_aux_t = train_data_a.edge_label_index.cpu().numpy().T,\\\n",
    "                                                       val_data_a.edge_label_index.cpu().numpy().T,\\\n",
    "                                                       test_data_a.edge_label_index.cpu().numpy().T\n",
    "\n",
    "train_edges_aux_t, val_edges_aux_t, test_edges_aux_t = [(train_edges_aux_t[i][0], train_edges_aux_t[i][1]) for i in range(len(train_edges_aux_t))],\\\n",
    "                                                       [(val_edges_aux_t[i][0], val_edges_aux_t[i][1]) for i in range(len(val_edges_aux_t))],\\\n",
    "                                                       [(test_edges_aux_t[i][0], test_edges_aux_t[i][1]) for i in range(len(test_edges_aux_t))]\n",
    "\n",
    "def get_aux_targets(train_edges_aux_t: list) -> list:\n",
    "    aux_train_target = []\n",
    "    for k in range(len(train_edges_aux_t)):\n",
    "        if train_edges_aux_t[k] in aux_target_dict:\n",
    "            aux_train_target.append(aux_target_dict[train_edges_aux_t[k]])\n",
    "        else:\n",
    "            aux_train_target.append([0, 0, 0, 0, 0])\n",
    "    return aux_train_target\n",
    "\n",
    "def task_split(aux_train_targets):\n",
    "    y_q, y_sjr, y_h, y_if, y_n = np.array(aux_train_targets).T\n",
    "    return torch.Tensor(y_q.T).float().cuda(),\\\n",
    "           torch.Tensor(y_sjr.T).float().cuda(),\\\n",
    "           torch.Tensor(y_h.T).float().cuda(),\\\n",
    "           torch.Tensor(y_if.T).float().cuda(),\\\n",
    "           torch.Tensor(y_n.T).float().cuda()\n",
    "\n",
    "aux_train_targets, aux_val_targets, aux_test_targets = get_aux_targets(train_edges_aux_t),\\\n",
    "                                                       get_aux_targets(val_edges_aux_t),\\\n",
    "                                                       get_aux_targets(test_edges_aux_t)\n",
    "\n",
    "train_aux_y, test_aux_y = task_split(aux_train_targets), task_split(aux_test_targets)\n",
    "\n",
    "train_data_a.aux = train_aux_y\n",
    "test_data_a.aux = test_aux_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228689ba",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResLinearBlock(nn.Module):\n",
    "    def __init__(self, size, link_size):\n",
    "        super(ResLinearBlock, self).__init__()\n",
    "        self.linear_1 = nn.Linear(size, size)\n",
    "        self.linear_2 = nn.Linear(size, size)\n",
    "        self.linear_3 = nn.Linear(size, link_size)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(size)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.linear_1(x)\n",
    "        result = F.relu(self.batch_norm_1(result))\n",
    "        result = self.linear_2(result)\n",
    "        result = F.relu(self.batch_norm_2(result))\n",
    "        result = self.linear_3(x + result)\n",
    "        return result\n",
    "    \n",
    "class gs_sum_concatenation_gs(nn.Module):\n",
    "    def __init__(self, data_c, \n",
    "                 parameters, \n",
    "                 train_data_a, val_data_a, test_data_a):\n",
    "        super(gs_sum_concatenation_gs, self).__init__()\n",
    "        \n",
    "        self.data_c  = data_c\n",
    "        self.train_data_a, self.val_data_a, self.test_data_a = train_data_a, val_data_a, test_data_a \n",
    "        self.params = parameters\n",
    "        \n",
    "        # convolutions on citation graph\n",
    "        self.conv_c_1 = GATv2Conv(data_c.x.shape[1], self.params[\"conv_size\"][0])\n",
    "        self.conv_c_2 = GATv2Conv(self.params[\"conv_size\"][0], self.params[\"conv_size\"][1])\n",
    "        self.conv_c_3 = GATv2Conv(self.params[\"conv_size\"][1], self.params[\"conv_size\"][2])\n",
    "        \n",
    "        # aggregation\n",
    "        self.pre_conv = nn.Linear(self.params[\"conv_size\"][2]+19, 75)\n",
    "            \n",
    "        self.input_size = self.params[\"conv_size\"][2]+19\n",
    "        self.num_layers = 1\n",
    "        self.hidden_size = 75\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=75, hidden_size=75,\n",
    "                    num_layers=1, batch_first=True)\n",
    "        \n",
    "        # convolutions on co-authorship graph\n",
    "        \n",
    "        self.convs_a = ModuleList()\n",
    "        self.batch_norms = ModuleList()\n",
    "        \n",
    "        for _ in range(3):\n",
    "            conv_a = GATv2Conv(75, 75)\n",
    "            self.convs_a.append(conv_a)\n",
    "            self.batch_norms.append(BatchNorm(75))\n",
    "        \n",
    "        # post link prediction layers\n",
    "        self.post_lp_layers = ModuleList()\n",
    "        for _ in range(4):\n",
    "            hidden_post_lp = nn.Linear(75, 1)\n",
    "            self.post_lp_layers.append(hidden_post_lp)\n",
    "            \n",
    "        # multitask\n",
    "        self.hidden_q1 = ResLinearBlock(75, 128)\n",
    "        self.hidden_q2 = nn.Linear(128, 75)\n",
    "        \n",
    "        self.hidden_if1 = ResLinearBlock(75, 128)\n",
    "        self.hidden_if2 = nn.Linear(128, 75)\n",
    "        \n",
    "        self.hidden_hi1 = ResLinearBlock(75, 128)\n",
    "        self.hidden_hi2 = nn.Linear(128, 75)\n",
    "\n",
    "        self.hidden_sjr1 = ResLinearBlock(75, 128)\n",
    "        self.hidden_sjr2 = nn.Linear(128, 75)\n",
    "\n",
    "    def forward(self, sample, batch_list_x, batch_list_owner, operator = \"l2\"):\n",
    "        def cp(z, edge_index):\n",
    "            return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
    "\n",
    "        def l1(z, edge_index):\n",
    "            return (torch.abs(z[edge_index[0]] - z[edge_index[1]]))\n",
    "\n",
    "        def l2(z, edge_index):\n",
    "            return (torch.pow(z[edge_index[0]] - z[edge_index[1]], 2))\n",
    "\n",
    "        def hadamard(z, edge_index):\n",
    "            return (z[edge_index[0]] * z[edge_index[1]])\n",
    "       \n",
    "        def summ(z, edge_index):\n",
    "            return (z[edge_index[0]] + z[edge_index[1]])\n",
    "        \n",
    "        x_a = self.train_data_a.x\n",
    "        if True:\n",
    "            x_c = F.relu(self.conv_c_1(self.data_c.x, self.data_c.edge_index, None))\n",
    "            x_c = F.relu(self.conv_c_2(x_c, self.data_c.edge_index, None))\n",
    "            x_c = F.relu(self.conv_c_3(x_c, self.data_c.edge_index, None))\n",
    "\n",
    "\n",
    "            counter = 0\n",
    "            x = torch.zeros(x_a.shape[0], x_a.shape[1] + x_c.shape[1]).to(device)\n",
    "            for i in range(len(x_a)):\n",
    "                if i in authors_to_papers:\n",
    "                    collab_emb = sum(x_c[list(authors_to_papers[i])])\n",
    "                else:\n",
    "                    counter += 1\n",
    "                    collab_emb = torch.zeros(self.params[\"conv_size\"][2]).to(device)\n",
    "                x[i] = torch.cat((x_a[i].unsqueeze(0), collab_emb.unsqueeze(0)), 1)\n",
    "\n",
    "        convolutions = []\n",
    "        x = self.pre_conv(x)\n",
    "        convolutions.append(x)\n",
    "        for conv, batch_norm in zip(self.convs_a, self.batch_norms):\n",
    "            x = F.relu(conv(x, self.train_data_a.edge_index))\n",
    "            convolutions.append(x)\n",
    "        \n",
    "        horizontal = []\n",
    "        for j in range(len(convolutions[0])):\n",
    "            horizontal.append([convolutions[i][j] for i in range(len(convolutions))])\n",
    "            horizontal[j] = torch.stack(horizontal[j])\n",
    "        emb_seqs_t = torch.stack(horizontal)\n",
    "\n",
    "        h_0 = Variable(torch.zeros(\n",
    "        self.num_layers, emb_seqs_t.size(0), self.hidden_size)).to(device)\n",
    "\n",
    "        c_0 = Variable(torch.zeros(\n",
    "        self.num_layers, emb_seqs_t.size(0), self.hidden_size)).to(device)\n",
    "\n",
    "        ula, (h_out, _) = self.lstm(emb_seqs_t, (h_0, c_0))        \n",
    "        \n",
    "        x = h_out.view(-1, self.hidden_size)\n",
    "        \n",
    "        q = self.hidden_q1(x)\n",
    "        q = self.hidden_q2(q)\n",
    "        \n",
    "        ifact = self.hidden_if1(x)\n",
    "        ifact = self.hidden_if2(ifact)\n",
    "        \n",
    "        hi = self.hidden_hi1(x)\n",
    "        hi = self.hidden_hi2(hi)\n",
    "        \n",
    "        sjr = self.hidden_sjr1(x)\n",
    "        sjr = self.hidden_sjr2(sjr)\n",
    "        \n",
    "        operator_dict = {\"cp\": cp, \"l1\": l1, \"l2\": l2, \"hadamard\": hadamard, \"summ\": summ}\n",
    "        embedding_operator = operator_dict[operator]\n",
    "        \n",
    "        edge_index = sample.edge_label_index\n",
    "        \n",
    "        link_embeddings, sjr_embeddings, hi_embeddings, if_embeddings = embedding_operator(x, edge_index),\\\n",
    "                                                                        embedding_operator(sjr, edge_index),\\\n",
    "                                                                        embedding_operator(hi, edge_index),\\\n",
    "                                                                        embedding_operator(ifact, edge_index)\n",
    "        \n",
    "        embeddings = [link_embeddings, sjr_embeddings, hi_embeddings, if_embeddings]\n",
    "        if embedding_operator != cp:\n",
    "            for i in range(len(embeddings)):\n",
    "                embeddings[i] = self.post_lp_layers[i](embeddings[i]).squeeze(-1)\n",
    "            \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "operator = \"hadamard\"\n",
    "\n",
    "def train(model, optimizer, criterion, mt_coeffs = [0, 0, 0]):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    sample = model.train_data_a\n",
    "    z, z_sjr, z_hi, z_ifact = model(sample, batch_list_x, batch_list_owner, operator)\n",
    "    edge_index = sample.edge_label_index\n",
    "#    link_embeddings = F.relu(final(link_embeddings))\n",
    "    link_labels = sample.edge_label\n",
    "    loss = F.binary_cross_entropy_with_logits(z, link_labels)\\\n",
    "                                               + mt_coeffs[0]*criterion(z_sjr, sample.aux[1])\\\n",
    "                                               + mt_coeffs[1]*criterion(z_hi, sample.aux[2])\\\n",
    "                                               + mt_coeffs[2]*criterion(z_ifact, sample.aux[3])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "    \n",
    "@torch.no_grad()\n",
    "def test(model, optimizer, criterion):\n",
    "    model.eval()\n",
    "    perfs = []\n",
    "    aux = []\n",
    "\n",
    "    for sample in [model.train_data_a, model.test_data_a]: \n",
    "        z, z_sjr, z_hi, z_ifact = model(sample, batch_list_x, batch_list_owner, operator)\n",
    "#        link_embeddings = F.relu(final(link_embeddings))\n",
    "        link_probs = z.sigmoid()\n",
    "        link_labels = sample.edge_label\n",
    "        aux.append([mean_absolute_error(sample.aux[1].cpu(), z_sjr.cpu()),\\\n",
    "                    mean_absolute_error(sample.aux[2].cpu(), z_hi.cpu()),\\\n",
    "                    mean_absolute_error(sample.aux[3].cpu(), z_ifact.cpu())])\n",
    "        perfs.append([accuracy_score(link_labels.cpu(), link_probs.cpu().round()),\\\n",
    "                      f1_score(link_labels.cpu(), link_probs.cpu().round()),\\\n",
    "                      roc_auc_score(link_labels.cpu(), link_probs.cpu())])\n",
    "    return perfs, aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c8566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(project_name, group, entity, mt_weights, model, optimizer, criterion, i):\n",
    "    \n",
    "    wandb.init(project=project_name, entity=entity, group=group) #, group=\"Experimental\")\n",
    "    wandb.run.name = group + \"_\" + str(i)\n",
    "    wandb.run.save()\n",
    "    \n",
    "    max_acc_test, max_f1_test, max_roc_auc_test = 0, 0, 0\n",
    "    max_acc_val, max_f1_val, max_roc_auc_val = 0, 0, 0\n",
    "\n",
    "    min_mae_sjr_test, min_mae_h_index_test, min_mae_impact_factor_test, min_mae_number_test = 100500, 100500, 100500, 100500\n",
    "    min_mae_sjr_val, min_mae_h_index_val, min_mae_impact_factor_val, min_mae_number_val = 100500, 100500, 100500, 100500\n",
    "    \n",
    "    for epoch in tqdm(range(epochs_per_launch)):\n",
    "        loss = []\n",
    "        train_loss = train(model, optimizer, criterion, mt_weights)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            metrics, metrics_aux = test(model, optimizer, criterion)\n",
    "            print(\"Loss:\", float(train_loss), \"\\nTrain:\", metrics[0], \"\\nTest:\", metrics[1])\n",
    "\n",
    "            print(\"Aux:\", metrics_aux)\n",
    "            \n",
    "        wandb.log({\"main_train/train_acc\":  metrics[0][0], \"main_test/test_acc\": metrics[1][0],\\\n",
    "                   \"main_train/train_f1\": metrics[0][1], \"main_test/test_f1\": metrics[1][1],\\\n",
    "                   \"main_train/train_roc_auc\": metrics[0][2], \"main_test/test_roc_auc\": metrics[1][2]})\n",
    "\n",
    "        if metrics[1][0] > max_acc_test:\n",
    "            max_acc_test = metrics[1][0]\n",
    "        if metrics[1][1] > max_f1_test:\n",
    "            max_f1_test = metrics[1][1]\n",
    "        if metrics[1][2] > max_roc_auc_test:\n",
    "            max_roc_auc_test = metrics[1][2]  \n",
    "        wandb.log({\"main_max/test_max_acc\": max_acc_test,\\\n",
    "                   \"main_max/test_max_f1\": max_f1_test,\\\n",
    "                   \"main_max/test_max_roc_auc\": max_roc_auc_test})\n",
    "        \"\"\"\n",
    "        if metrics[1][0] > max_acc_val:\n",
    "            max_acc_val = metrics[2][0]\n",
    "        if metrics[1][1] > max_f1_val:\n",
    "            max_f1_val = metrics[2][1]\n",
    "        if metrics[1][2] > max_roc_auc_val:\n",
    "            max_roc_auc_val = metrics[2][2]  \n",
    "        wandb.log({\"val_max_acc\": max_acc_val,\\\n",
    "                   \"val_max_f1\": max_f1_val,\\\n",
    "                   \"val_max_roc_auc\": max_roc_auc_val})\n",
    "        \"\"\"\n",
    "\n",
    "        wandb.log({\"aux_train/train_mae_sjr\":  metrics_aux[0][0], \"aux_train/train_mae_h_index\": metrics_aux[0][1], \"aux_train/train_mae_impact_factor\": metrics_aux[0][2],\n",
    "                   \"aux_test/test_mae_sjr\":  metrics_aux[1][0], \"aux_test/test_mae_h_index\": metrics_aux[1][1], \"aux_test/test_mae_impact_factor\": metrics_aux[1][2],})\n",
    "\n",
    "        if metrics_aux[1][0] < min_mae_sjr_test:\n",
    "            min_mae_sjr_test = metrics_aux[1][0]\n",
    "        if metrics_aux[1][1] < min_mae_h_index_test:\n",
    "            min_mae_h_index_test = metrics_aux[1][1]\n",
    "        if metrics_aux[1][2] < min_mae_impact_factor_test:\n",
    "            min_mae_impact_factor_test = metrics_aux[1][2]  \n",
    "        wandb.log({\"aux_min/test_min_mae_sjr\": min_mae_sjr_test,\\\n",
    "                   \"aux_min/test_min_mae_h_index\": min_mae_h_index_test,\\\n",
    "                   \"aux_min/test_min_mae_impact_factor\": min_mae_impact_factor_test})\n",
    "        \"\"\"\n",
    "        if metrics_aux[1][0] <  min_mae_sjr_val:\n",
    "            min_mae_sjr_val = metrics_aux[1][0]\n",
    "        if metrics_aux[1][1] < min_mae_h_index_val:\n",
    "            min_mae_h_index_val = metrics_aux[1][1]\n",
    "        if metrics_aux[1][2] < min_mae_impact_factor_val:\n",
    "            min_mae_impact_factor_val = metrics_aux[1][2]  \n",
    "        wandb.log({\"test_min_mae_sjr\": min_mae_sjr_val,\\\n",
    "                   \"test_min_mae_h_index\": min_mae_h_index_val,\\\n",
    "                   \"test_min_mae_impact_factor\": min_mae_impact_factor_val})\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b2a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'GAT_0.1'\n",
    "epochs_per_launch = 15000\n",
    "lr = 0.0005\n",
    "mt_weights = [[0.3, 0.3, 0.3]]\n",
    "for i in range(10):\n",
    "    for mt_weight in mt_weights:\n",
    "        group = \"rmgnn(a_original_rgat_res)_split_n0_\" + str(mt_weight[0]) + \"_\"\\\n",
    "        + str(mt_weight[1]) + \"_\" + str(mt_weight[2]) + \"_\"\\\n",
    "        + operator + \"_\" + str(lr) + \"_no_wd\"\n",
    "        entity = \"sbergraphs\"\n",
    "        device = 'cuda:1' # torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        parameters = {\"conv_size\": [128, 128, 128]}\n",
    "        model = gs_sum_concatenation_gs(data_citation, parameters,\\\n",
    "                                    train_data_a, val_data_a, test_data_a).to(device) \n",
    "        data_citation, data_author, train_data_a = data_citation.to(device), data_author.to(device), train_data_a.to(device) \n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.L1Loss()\n",
    "        run(project_name, group, entity, mt_weight, model, optimizer, criterion, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

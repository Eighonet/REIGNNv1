{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70189996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f69efcd",
   "metadata": {},
   "source": [
    "General data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ffaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_2_root = \"../\"\n",
    "authors_edges_papers_general = pd.read_csv(path_2_root + \"processed_data/SSORC_CS_2010_2021_authors_edges_papers_indices.csv\", index_col = 0, \\\n",
    "                                   converters={\"papers_indices\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "authors_edges_general = pd.read_csv(path_2_root + \"processed_data/SSORC_CS_2010_2021_authors_edge_list.csv\", index_col = 0)\n",
    "papers_features_general = pd.read_csv(path_2_root + \"processed_data/SSORC_CS_2010_2021_papers_features_vectorized_compressed_32.csv\", index_col = 0)\n",
    "authors_features_general = pd.read_csv(path_2_root + \"processed_data/SSORC_CS_2010_2021_authors_features.csv\", index_col = 0)\n",
    "aev = authors_edges_general.values\n",
    "edge_to_index = {(aev[i][0], aev[i][1]):i for i in tqdm(range(len(aev)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea672d96",
   "metadata": {},
   "source": [
    "Local data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ee54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'SSORC_CS_10_21_1437_3164_unfiltered'\n",
    "\n",
    "authors_edges_papers = pd.read_csv(path_2_root + \"datasets/\" + dataset_name + \"/\" + dataset_name + \"_\" + \"authors_edges_papers_indices.csv\", index_col = 0,\\\n",
    "                                   converters={\"papers_indices\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "authors_graph = nx.read_edgelist(path_2_root + \"datasets/\" + dataset_name + \"/\" + dataset_name + \"_\" + \"authors.edgelist\", create_using = nx.DiGraph)\n",
    "citation_graph = nx.read_edgelist(path_2_root + \"datasets/\" + dataset_name + \"/\" + dataset_name + \"_\" + \"papers.edgelist\", create_using = nx.DiGraph)\n",
    "papers_targets = pd.read_csv(path_2_root + \"datasets/\" + dataset_name + \"/\" + dataset_name + \"_papers_targets.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331862c2",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeab5ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = \"5_0.1\"\n",
    "path = path_2_root + \"datasets/\" + dataset_name + \"/split_\" + str(splits) + \"/\"  \n",
    "\n",
    "split = 4\n",
    "train_data_a = torch.load(path + dataset_name + '_train_sample_' + str(split) + '.data')\n",
    "val_data_a = torch.load(path + dataset_name + '_val_sample_' + str(split) + '.data')\n",
    "test_data_a = torch.load(path + dataset_name + '_test_sample_' + str(split) + '.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa77b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_nodes = list(citation_graph.nodes)\n",
    "papers_nodes = [int(papers_nodes[i]) for i in range(len(papers_nodes))]\n",
    "papers_node_features = papers_features_general.iloc[papers_nodes, :]\n",
    "for node in tqdm(citation_graph.nodes):\n",
    "    citation_graph.nodes[node]['x'] = list(papers_node_features.loc[[int(node)]].values[0])\n",
    "authors_nodes = list(authors_graph.nodes)\n",
    "authors_nodes = [int(authors_nodes[i]) for i in range(len(authors_nodes))]\n",
    "authors_node_features = authors_features_general.loc[authors_nodes]\n",
    "for node in tqdm(authors_graph.nodes):\n",
    "    authors_graph.nodes[node]['x'] = list(authors_node_features.loc[[int(node)]].values[0])\n",
    "data_author = from_networkx(authors_graph)\n",
    "data_citation = from_networkx(citation_graph)\n",
    "\n",
    "train_data_a.x, val_data_a.x, test_data_a.x = data_author.x.float(), data_author.x.float(), data_author.x.float()\n",
    "data_citation.x = data_citation.x.float()\n",
    "\n",
    "original_a_nodes = list(authors_graph.nodes)\n",
    "pyg_id_2_original_id = {i:int(original_a_nodes[i]) for i in range(len(original_a_nodes))}\n",
    "\n",
    "\n",
    "original_a_nodes = list(authors_graph.nodes)\n",
    "pyg_id_2_original_id = {i:int(original_a_nodes[i]) for i in range(len(original_a_nodes))}\n",
    "\n",
    "sAe_t = train_data_a.edge_index.cpu().numpy().T\n",
    "sAe_t = [(pyg_id_2_original_id[int(sAe_t[i][0])], pyg_id_2_original_id[int(sAe_t[i][1])]) for i in range(len(sAe_t))]\n",
    "\n",
    "authors_edges_papers_sub_2t = [authors_edges_papers[\"papers_indices\"][edge_to_index[sAe_t[i]]] for i in tqdm(range(len(sAe_t)))]\n",
    "authors_edges_papers_sub_flat_2t = [str(item) for subarray in authors_edges_papers_sub_2t for item in subarray]\n",
    "unique_papers_2t = list(set(authors_edges_papers_sub_flat_2t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee6c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_graph_sub = citation_graph.subgraph(unique_papers_2t)\n",
    "citation_graph_sub_nodes = list(citation_graph_sub.nodes())\n",
    "global_to_local_id_citation = {citation_graph_sub_nodes[i]:i for i in range(len(citation_graph_sub_nodes))}\n",
    "authors_graph_sub_nodes = list(authors_graph.nodes())\n",
    "global_to_local_id_authors = {authors_graph_sub_nodes[i]:i for i in range(len(authors_graph_sub_nodes))}\n",
    "\n",
    "authors_to_papers = dict()\n",
    "for i in tqdm(range(len(sAe_t))):\n",
    "    papers = authors_edges_papers_sub_2t[i]\n",
    "    author_1, author_2 = sAe_t[i]\n",
    "    for author in sAe_t[i]:\n",
    "        if author in authors_to_papers:\n",
    "            for paper in papers:\n",
    "                authors_to_papers[global_to_local_id_authors[str(author)]].add(global_to_local_id_citation[paper])\n",
    "        else:\n",
    "            authors_to_papers[global_to_local_id_authors[str(author)]] = set()\n",
    "            for paper in papers:\n",
    "                authors_to_papers[global_to_local_id_authors[str(author)]].add(global_to_local_id_citation[paper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c8c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in tqdm(citation_graph_sub.nodes):\n",
    "    citation_graph_sub.nodes[node]['x'] = list(papers_features_general.loc[[int(node)]].values[0])\n",
    "data_citation = from_networkx(citation_graph_sub)\n",
    "data_citation.x = data_citation.x.float()\n",
    "\n",
    "authors_nodes = list(authors_graph.nodes)\n",
    "authors_nodes = [int(authors_nodes[i]) for i in range(len(authors_nodes))]\n",
    "authors_node_features = authors_features_general.loc[authors_nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0238a5d",
   "metadata": {},
   "source": [
    "### Auxiliary targets computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eda485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_author = from_networkx(authors_graph)\n",
    "edges_ordered = [(int(data_author.edge_index.T[i][0]), int(data_author.edge_index.T[i][1])) for i in range(len(data_author.edge_index.T))]\n",
    "index_to_edge = {i:edges_ordered[i] for i in range(len(edges_ordered))}\n",
    "authors_edges_papers_sample = authors_edges_papers_sub_2t\n",
    "citation_nodes = list(citation_graph_sub.nodes)\n",
    "ownership_dict = {}\n",
    "inds_dict = {}\n",
    "for i in tqdm(range(len(authors_edges_papers_sample))):\n",
    "    arr = authors_edges_papers_sample[i]\n",
    "    collab_embeddings = []\n",
    "    for j in range(len(arr)):\n",
    "        ind = citation_nodes.index(arr[j]) # index_outer_2_index_inner[int(arr[j])]\n",
    "        collab_embeddings.append(ind)\n",
    "    ownership_dict[i] = i\n",
    "    inds_dict[i] = collab_embeddings\n",
    "    \n",
    "embs_dict = inds_dict\n",
    "lens = set([len(embs_dict[i]) for i in range(len(embs_dict))])\n",
    "batch_dict_x = {}\n",
    "batch_dict_owner = {}\n",
    "batch_dict_ind = {}\n",
    "for i in tqdm(range(len(embs_dict))):\n",
    "    if (len(embs_dict[i])) in batch_dict_x:\n",
    "        batch_dict_x[len(embs_dict[i])].append(embs_dict[i])\n",
    "        batch_dict_owner[len(embs_dict[i])].append(ownership_dict[i])\n",
    "        batch_dict_ind[len(embs_dict[i])].append(i)\n",
    "    else:\n",
    "        batch_dict_x[len(embs_dict[i])], batch_dict_owner[len(embs_dict[i])], batch_dict_ind[len(embs_dict[i])] = [], [], []\n",
    "        batch_dict_x[len(embs_dict[i])].append(embs_dict[i])\n",
    "        batch_dict_owner[len(embs_dict[i])].append(ownership_dict[i])\n",
    "        batch_dict_ind[len(embs_dict[i])].append(i)\n",
    "\n",
    "for length in batch_dict_owner:\n",
    "    batch_dict_owner[length] = [index_to_edge[batch_dict_owner[length][i]] for i in range(len(batch_dict_owner[length]))]\n",
    "    \n",
    "batch_list_x = list(batch_dict_x.values())\n",
    "batch_list_owner = list(batch_dict_owner.values())\n",
    "batch_list_ind = list(batch_dict_ind.values())\n",
    "\n",
    "papers_targets = papers_targets.values\n",
    "aux_targets = []\n",
    "for i in tqdm(range(len(batch_list_x))):\n",
    "    batch = batch_list_x[i]\n",
    "    values = []\n",
    "    for j in range(len(batch)):\n",
    "        values = [papers_targets[batch[j][k]] for k in range(len(batch[j]))]\n",
    "        values = np.array(values).T\n",
    "        targets = [max(values[0]), sum(values[1])/len(values[1]), \n",
    "                   sum(values[2])/len(values[2]), sum(values[3])/len(values[3]),\n",
    "                   len(values[0])]\n",
    "        aux_targets.append(targets)\n",
    "        \n",
    "batch_list_owner_flat = [edge for batch in batch_list_owner for edge in batch]\n",
    "aux_target_dict = {batch_list_owner_flat[i]:aux_targets[i] for i in tqdm(range(len(aux_targets)))}\n",
    "\n",
    "train_edges_aux_t, val_edges_aux_t, test_edges_aux_t = train_data_a.edge_label_index.cpu().numpy().T,\\\n",
    "                                                       val_data_a.edge_label_index.cpu().numpy().T,\\\n",
    "                                                       test_data_a.edge_label_index.cpu().numpy().T\n",
    "\n",
    "train_edges_aux_t, val_edges_aux_t, test_edges_aux_t = [(train_edges_aux_t[i][0], train_edges_aux_t[i][1]) for i in range(len(train_edges_aux_t))],\\\n",
    "                                                       [(val_edges_aux_t[i][0], val_edges_aux_t[i][1]) for i in range(len(val_edges_aux_t))],\\\n",
    "                                                       [(test_edges_aux_t[i][0], test_edges_aux_t[i][1]) for i in range(len(test_edges_aux_t))]\n",
    "\n",
    "def get_aux_targets(train_edges_aux_t: list) -> list:\n",
    "    aux_train_target = []\n",
    "    for k in range(len(train_edges_aux_t)):\n",
    "        if train_edges_aux_t[k] in aux_target_dict:\n",
    "            aux_train_target.append(aux_target_dict[train_edges_aux_t[k]])\n",
    "        else:\n",
    "            aux_train_target.append([0, 0, 0, 0, 0])\n",
    "    return aux_train_target\n",
    "\n",
    "def task_split(aux_train_targets):\n",
    "    y_q, y_sjr, y_h, y_if, y_n = np.array(aux_train_targets).T\n",
    "    return torch.Tensor(y_q.T).float().cuda(),\\\n",
    "           torch.Tensor(y_sjr.T).float().cuda(),\\\n",
    "           torch.Tensor(y_h.T).float().cuda(),\\\n",
    "           torch.Tensor(y_if.T).float().cuda(),\\\n",
    "           torch.Tensor(y_n.T).float().cuda()\n",
    "\n",
    "aux_train_targets, aux_val_targets, aux_test_targets = get_aux_targets(train_edges_aux_t),\\\n",
    "                                                       get_aux_targets(val_edges_aux_t),\\\n",
    "                                                       get_aux_targets(test_edges_aux_t)\n",
    "\n",
    "train_aux_y, test_aux_y = task_split(aux_train_targets), task_split(aux_test_targets)\n",
    "\n",
    "train_data_a.aux = train_aux_y\n",
    "test_data_a.aux = test_aux_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bfe41",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from REIGNN import REIGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f9dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B parameters\n",
    "project_name = 'GAT_0.3'\n",
    "prefix = \"standard_aux_st_convs\"\n",
    "entity = \"sbergraphs\"\n",
    "wandb_output = False\n",
    "\n",
    "# Global\n",
    "epochs_per_launch = 15000\n",
    "lr = 0.001\n",
    "device = 'cuda:0'\n",
    "\n",
    "# Local\n",
    "heads = 1\n",
    "\n",
    "c_conv_num = 2\n",
    "c_latent_size = 128\n",
    "\n",
    "a_conv_num = 3\n",
    "a_latent_size = 384\n",
    "\n",
    "operator = \"hadamard\"\n",
    "link_size = 128\n",
    "\n",
    "# Multitask weights\n",
    "mt_weights = [[0.05, 0.05, 0.05, 0.05]]\n",
    "\n",
    "def train(model, optimizer, criterion, mt_coeffs = [0, 0, 0, 0], main_coeff = 0.1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    sample = model.train_data_a\n",
    "    z, z_sjr, z_hi, z_ifact, z_numb = model(sample, batch_list_x, batch_list_owner, operator)\n",
    "    edge_index = sample.edge_label_index\n",
    "#    link_embeddings = F.relu(final(link_embeddings))\n",
    "    link_labels = sample.edge_label\n",
    "    main_loss = main_coeff*F.binary_cross_entropy_with_logits(z, link_labels)\n",
    "    mask = z > 0\n",
    "    loss = main_loss\\\n",
    "           + mt_coeffs[0]*criterion(z_sjr[mask], sample.aux[1][mask])\\\n",
    "           + mt_coeffs[1]*criterion(z_hi[mask], sample.aux[2][mask])\\\n",
    "           + mt_coeffs[2]*criterion(z_ifact[mask], sample.aux[3][mask])\\\n",
    "           + mt_coeffs[3]*criterion(z_ifact[mask], sample.aux[4][mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "    \n",
    "@torch.no_grad()\n",
    "def test(model, optimizer, criterion):\n",
    "    model.eval()\n",
    "    perfs = []\n",
    "    aux = []\n",
    "\n",
    "    for sample in [model.train_data_a, model.test_data_a]: \n",
    "        z, z_sjr, z_hi, z_ifact, z_numb = model(sample, batch_list_x, batch_list_owner, operator)\n",
    "#        link_embeddings = F.relu(final(link_embeddings))\n",
    "        link_probs = z.sigmoid()\n",
    "        link_labels = sample.edge_label\n",
    "        aux.append([mean_absolute_error(sample.aux[1].cpu(), z_sjr.cpu()),\\\n",
    "                    mean_absolute_error(sample.aux[2].cpu(), z_hi.cpu()),\\\n",
    "                    mean_absolute_error(sample.aux[3].cpu(), z_ifact.cpu()),\\\n",
    "                    mean_absolute_error(sample.aux[4].cpu(), z_numb.cpu())])\n",
    "        perfs.append([accuracy_score(link_labels.cpu(), link_probs.cpu().round()),\\\n",
    "                      f1_score(link_labels.cpu(), link_probs.cpu().round()),\\\n",
    "                      roc_auc_score(link_labels.cpu(), link_probs.cpu())])\n",
    "    return perfs, aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b0273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(project_name, group, entity, mt_weights, model, optimizer, criterion, i):\n",
    "    if wandb_output:\n",
    "        wandb.init(project=project_name, entity=entity, group=group) #, group=\"Experimental\")\n",
    "        wandb.run.name = group + \"_\" + str(i)\n",
    "        wandb.run.save()\n",
    "    \n",
    "    max_acc_test, max_f1_test, max_roc_auc_test = 0, 0, 0\n",
    "    max_acc_val, max_f1_val, max_roc_auc_val = 0, 0, 0\n",
    "\n",
    "    min_mae_sjr_test, min_mae_h_index_test, min_mae_impact_factor_test, min_mae_number_test = 100500, 100500, 100500, 100500\n",
    "    min_mae_sjr_val, min_mae_h_index_val, min_mae_impact_factor_val, min_mae_number_val = 100500, 100500, 100500, 100500\n",
    "    \n",
    "    for epoch in tqdm(range(epochs_per_launch)):\n",
    "        loss = []\n",
    "        train_loss = train(model, optimizer, criterion, mt_weights)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            metrics, metrics_aux = test(model, optimizer, criterion)\n",
    "            print(\"Train main:\", metrics[0])\n",
    "            print(\"Test main:\", metrics[1])\n",
    "            print(\"Train auxiliary:\", metrics_aux[0])\n",
    "            print(\"Test auxiliary:\", metrics_aux[1])\n",
    "        \n",
    "        if metrics[1][0] > max_acc_test:\n",
    "            max_acc_test = metrics[1][0]\n",
    "        if metrics[1][1] > max_f1_test:\n",
    "            max_f1_test = metrics[1][1]\n",
    "        if metrics[1][2] > max_roc_auc_test:\n",
    "            max_roc_auc_test = metrics[1][2]  \n",
    "        \n",
    "        if metrics_aux[1][0] < min_mae_sjr_test:\n",
    "            min_mae_sjr_test = metrics_aux[1][0]\n",
    "        if metrics_aux[1][1] < min_mae_h_index_test:\n",
    "            min_mae_h_index_test = metrics_aux[1][1]\n",
    "        if metrics_aux[1][2] < min_mae_impact_factor_test:\n",
    "            min_mae_impact_factor_test = metrics_aux[1][2]  \n",
    "        if metrics_aux[1][3] < min_mae_number_test:\n",
    "            min_mae_number_test = metrics_aux[1][3]\n",
    "            \n",
    "        if wandb_output:\n",
    "            wandb.log({\"main_train/train_acc\":  metrics[0][0], \"main_test/test_acc\": metrics[1][0],\\\n",
    "                   \"main_train/train_f1\": metrics[0][1], \"main_test/test_f1\": metrics[1][1],\\\n",
    "                   \"main_train/train_roc_auc\": metrics[0][2], \"main_test/test_roc_auc\": metrics[1][2]})\n",
    "            \n",
    "            wandb.log({\"main_max/test_max_acc\": max_acc_test,\\\n",
    "                   \"main_max/test_max_f1\": max_f1_test,\\\n",
    "                   \"main_max/test_max_roc_auc\": max_roc_auc_test})\n",
    "        \n",
    "            wandb.log({\"aux_train/train_mae_sjr\":  metrics_aux[0][0], \"aux_train/train_mae_h_index\": metrics_aux[0][1], \"aux_train/train_mae_impact_factor\": metrics_aux[0][2], \"aux_train/train_number\": metrics_aux[0][3],\n",
    "                   \"aux_test/test_mae_sjr\":  metrics_aux[1][0], \"aux_test/test_mae_h_index\": metrics_aux[1][1], \"aux_test/test_mae_impact_factor\": metrics_aux[1][2], \"aux_test/test_number\": metrics_aux[1][3]})\n",
    "            \n",
    "            wandb.log({\"aux_min/test_min_mae_sjr\": min_mae_sjr_test,\\\n",
    "                   \"aux_min/test_min_mae_h_index\": min_mae_h_index_test,\\\n",
    "                   \"aux_min/test_min_mae_impact_factor\": min_mae_impact_factor_test,\n",
    "                   \"aux_min/test_min_number_factor\": min_mae_number_test})\n",
    "            \n",
    "for i in range(10):\n",
    "    for mt_weight in mt_weights:\n",
    "        group = \"rmgnn(a_rgat(\" + prefix + \"_\" + str(link_size) +  \"_\" + str(a_latent_size) + \"_\" + str(a_conv_num) + \"_\" + str(c_conv_num) + \"_\" + str(c_latent_size) + \")_v2)_split_n0_\" + str(mt_weight[0]) + \"_\"\\\n",
    "        + str(mt_weight[1]) + \"_\" + str(mt_weight[2]) + \"_\" + str(mt_weight[3])\\\n",
    "        + operator + \"_\" + str(lr) + \"_no_wd\"\n",
    "        model = REIGNN(data_citation, heads, device,\\\n",
    "                                    train_data_a, val_data_a, test_data_a,\n",
    "                                    authors_to_papers,\n",
    "                                    cit_layers = c_conv_num, latent_size_cit = c_latent_size,\n",
    "                                    auth_layers = a_conv_num, latent_size_auth = a_latent_size,\n",
    "                                    link_size = link_size).to(device) \n",
    "        data_citation, data_author, train_data_a = data_citation.to(device), data_author.to(device), train_data_a.to(device) \n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.L1Loss()\n",
    "        run(project_name, group, entity, mt_weight, model, optimizer, criterion, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f159b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
